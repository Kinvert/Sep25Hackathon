[base]
package = ocean
env_name = puffer_drone_pp
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[vec]
num_envs = 24

[env]
num_envs = 24 # 16
num_drones = 64 # 64
max_rings = 10

box_base_density = 50.0
box_k_growth = 0.382745113745057

dist_decay = 41.4607043293644

grip_k_decay = 3
grip_k_max = 13.176817276380092
grip_k_min = 1.0

pos_const = 0.6139523971036905
pos_penalty = 0.0001

reward_grip = 0.7504388912014437
reward_ho_drop = 0.2621607720500283
reward_hover = 0.25189232876140244

reward_max_dist = 65.0
reward_min_dist = 1.2574298048547647

vel_penalty_clamp = 0.45318093362341383

w_approach = 2.5
w_position = 0.830661256270675
w_stability = 0.3268312145336416
w_velocity = 0.2300285842499415

[train]
adam_beta1 = 0.9447721162246707
adam_beta2 = 0.9981486212852817
adam_eps = 3.017550458839966e-07
anneal_lr = true
batch_size = auto
bptt_horizon = 64
checkpoint_interval = 200
clip_coef = 0.3526717690896353
ent_coef = 0.022746615993432043
gae_lambda = 0.995
gamma = 0.9915404966826538
learning_rate = 0.01000000000000001
#learning_rate = 0.005
max_grad_norm = 2.5843397392403937
max_minibatch_size = 65536
minibatch_size = 32768
prio_alpha = 0.712012072750674
prio_beta0 = 0.99
total_timesteps = 1_000_000_000
update_epochs = 1
#use_rnn = false
vf_clip_coef = 0.1
vf_coef = 4.385961990267164
vtrace_c_clip = 1.2065773508911912
vtrace_rho_clip = 0.9726251246653761

[sweep]
method = Protein
metric = perfect_deliv
goal = maximize
downsample = 0

[sweep.env.box_k_growth]
distribution = uniform
min = 0.20
max = 1.0
mean = 0.382745113745057
scale = auto

[sweep.env.dist_decay]
distribution = uniform
min = 25.0
max = 60.0
mean = 41.4607043293644
scale = auto

[sweep.env.grip_k_decay]
distribution = uniform
min = 3.0
max = 15.0
mean = 3.0001
scale = auto

[sweep.env.grip_k_max]
distribution = uniform
min = 10.0
max = 20.0
mean = 13.176817276380092
scale = auto

[sweep.env.pos_const]
distribution = uniform
min = 0.25
max = 1.0
mean = 0.6139523971036905
scale = auto

[sweep.env.pos_penalty]
distribution = uniform
min = 0.0001
max = 0.25
mean = 0.0001
scale = auto

[sweep.env.reward_grip]
distribution = uniform
min = 0.25
max = 1.0
mean = 0.7504388912014437
scale = auto

[sweep.env.reward_ho_drop]
distribution = uniform
min = 0.1
max = 0.4
mean = 0.2621607720500283
scale = auto

[sweep.env.reward_hover]
distribution = uniform
min = 0.1
max = 0.3
mean = 0.25189232876140244
scale = auto

[sweep.env.reward_min_dist]
distribution = uniform
min = 0.1
max = 5.0
mean = 1.2574298048547647
scale = auto

[sweep.env.vel_penalty_clamp]
distribution = uniform
min = 0.1
max = 0.5
mean = 0.45318093362341383
scale = auto

[sweep.env.w_approach]
distribution = uniform
min = 0.5
max = 3.0
mean = 2.4999
scale = auto

[sweep.env.w_position]
distribution = uniform
min = 0.5
max = 1.5
mean = 0.830661256270675
scale = auto

[sweep.env.w_stability]
distribution = uniform
min = 0.0
max = 2.5
mean = 0.3268312145336416
scale = auto

[sweep.env.w_velocity]
distribution = uniform
min = 0.1
max = 1.5
mean = 0.2300285842499415
scale = auto
