[base]
package = ocean
env_name = puffer_drone_pp
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[vec]
num_envs = 8

[env]
num_envs = 24 # 16
num_drones = 64 # 64
max_rings = 10

penalty_damping = 0.065 #0.21 #0.1
reward_xy_dist = 0.09 #0.12 #0.05
reward_hover_dist = 0.88 #0.067 #0.15
reward_hover_alt = 0.031 #0.11 #0.03
reward_hover = 0.22 #0.2
reward_maint_hover = 0.12 #0.1
reward_descent = 0.75 #0.75
penalty_lost_hover = 0.075 #0.1
alignment = 0.001
min_alignment = 0.2
max_alignment = 0.001

reward_min_dist = 4.5
reward_max_dist = 80.0
dist_decay = 0.15

w_position = 1.0
w_velocity = 1.0
w_stability = 1.0
w_approach = 1.3
w_hover = 1.0

pos_const = 0.3
pos_penalty = 0.1

grip_k_min = 1.0
grip_k_max = 10.0
grip_k_decay = 0.05

[train]
adam_beta1 = 0.9610890980775877
adam_beta2 = 0.9999260775286266
adam_eps = 7.782906079040132e-10
anneal_lr = true
batch_size = auto
bptt_horizon = 64
checkpoint_interval = 200
clip_coef = 0.05982655642208556
ent_coef = 0.002465076521024325
gae_lambda = 0.9641173414828333
gamma = 0.997472126425902
learning_rate = 0.010933756713881205
#learning_rate = 0.005
max_grad_norm = 1.6317688647793107
max_minibatch_size = 32768
minibatch_size = 32768
prio_alpha = 0.8968873016577552
prio_beta0 = 0.8672928227817938
total_timesteps = 200_000_000
update_epochs = 1
#use_rnn = false
vf_clip_coef = 0.5869845581530236
vf_coef = 2.1319065538539963
vtrace_c_clip = 2.714930379733876
vtrace_rho_clip = 3.8183814893708057

[sweep]
method = Protein
metric = gripping
goal = maximize
downsample = 0

[sweep.env.w_position]
distribution = uniform
min = 0.0
max = 2.0
mean = 1.0
scale = auto

[sweep.env.w_velocity]
distribution = uniform
min = 0.0
max = 2.0
mean = 1.0
scale = auto

[sweep.env.w_stability]
distribution = uniform
min = 0.0
max = 2.0
mean = 1.0
scale = auto

[sweep.env.w_approach]
distribution = uniform
min = 0.0
max = 2.0
mean = 1.3
scale = auto

[sweep.env.w_hover]
distribution = uniform
min = 0.0
max = 2.0
mean = 1.0
scale = auto

[sweep.env.reward_min_dist]
distribution = uniform
min = 0.1
max = 10.0
mean = 4.5
scale = auto

[sweep.env.reward_max_dist]
distribution = uniform
min = 10.0
max = 100.0
mean = 80.0
scale = auto

[sweep.env.dist_decay]
distribution = uniform
min = 0.01
max = 0.5
mean = 0.15
scale = auto

[sweep.env.pos_const]
distribution = uniform
min = 0.001
max = 1.0
mean = 0.3
scale = auto

[sweep.env.pos_penalty]
distribution = uniform
min = 0.001
max = 0.2
mean = 0.1
scale = auto

[sweep.env.grip_k_min]
distribution = uniform
min = 0.001
max = 5.0
mean = 1.0
scale = auto

[sweep.env.grip_k_max]
distribution = uniform
min = 1.0
max = 20.0
mean = 10.0
scale = auto

[sweep.env.grip_k_decay]
distribution = uniform
min = 0.001
max = 1.0
mean = 0.05
scale = auto

#[sweep.env.alignment]
#distribution = uniform
#min = 0.0001
#max = 0.1
#mean = 0.001
#scale = auto

#[sweep.env.min_alignment]
#distribution = uniform
#min = 0.01
#max = 0.25
#mean = 0.2
#scale = auto

#[sweep.env.max_alignment]
#distribution = uniform
#min = 0.001
#max = 0.1
#mean = 0.01
#scale = auto

#[sweep.env.penalty_damping]
#distribution = uniform
#min = 0.001
#max = 0.5
#mean = 0.21
#scale = auto

#[sweep.env.reward_xy_dist]
#distribution = uniform
#min = 0.001
#max = 0.2
#mean = 0.12
#scale = auto

#[sweep.env.reward_hover_dist]
#distribution = uniform
#min = 0.001
#max = 0.2
#mean = 0.067
#scale = auto

#[sweep.env.reward_hover_alt]
#distribution = uniform
#min = 0.001
#max = 0.2
#mean = 0.11
#scale = auto

#[sweep.env.reward_hover]
#distribution = uniform
#min = 0.05
#max = 0.5
#mean = 0.2
#scale = auto

#[sweep.env.reward_maint_hover]
#distribution = uniform
#min = 0.05
#max = 0.2
#mean = 0.1
#scale = auto

#[sweep.env.reward_descent]
#distribution = uniform
#min = 0.2
#max = 1.0
#mean = 0.75
#scale = auto

#[sweep.env.penalty_lost_hover]
#distribution = uniform
#min = 0.01
#max = 0.2
#mean = 0.1
#scale = auto

#[sweep.train.total_timesteps]
#distribution = log_normal
#min = 2e8
#max = 4e8
#mean = 2e8
#scale = time
