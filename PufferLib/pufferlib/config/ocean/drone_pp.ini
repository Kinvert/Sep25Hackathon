[base]
package = ocean
env_name = puffer_drone_pp
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[vec]
num_envs = 8

[env]
num_envs = 16
num_drones = 64 # 64
max_rings = 10

penalty_damping = 0.21 #0.1     (POS CORR)
reward_hover_dist = 0.067 #0.15 (NEG CORR)
reward_xy_dist = 0.12 #0.05     (NEG CORR)
reward_hover_alt = 0.11 #0.03   (NEG CORR)
# pp0 denim-glade-1
    #0.1
    #0.15
    #0.05
    #0.15
# pp0 noble-water-28 n6g84fj7
    #0.21 (BIG) 75%
    #0.067 (SMALL) almost the smallest
    #0.12 (BIG) 75%
    #0.11 (SMALL) a bit small

[train]
adam_beta1 = 0.9610890980775877
adam_beta2 = 0.9999260775286266
adam_eps = 7.782906079040132e-10
anneal_lr = true
batch_size = auto
bptt_horizon = 64
checkpoint_interval = 200
clip_coef = 0.05982655642208556
ent_coef = 0.002465076521024325
gae_lambda = 0.9641173414828333
gamma = 0.997472126425902
learning_rate = 0.010933756713881205
#learning_rate = 0.005
max_grad_norm = 1.6317688647793107
max_minibatch_size = 32768
minibatch_size = 32768
prio_alpha = 0.8968873016577552
prio_beta0 = 0.8672928227817938
total_timesteps = 400_000_000 # =====================================
update_epochs = 1
#use_rnn = false
vf_clip_coef = 0.5869845581530236
vf_coef = 2.1319065538539963
vtrace_c_clip = 2.714930379733876
vtrace_rho_clip = 3.8183814893708057

[sweep]
method = Protein
metric = de_pickup
goal = maximize
downsample = 0

[sweep.env.penalty_damping]
distribution = uniform
min = 0.001
max = 0.5
mean = 0.1
scale = auto

[sweep.env.reward_hover_dist]
distribution = uniform
min = 0.001
max = 0.2
mean = 0.15
scale = auto

[sweep.env.reward_xy_dist]
distribution = uniform
min = 0.001
max = 0.2
mean = 0.05
scale = auto

[sweep.env.reward_hover_alt]
distribution = uniform
min = 0.001
max = 0.2
mean = 0.15
scale = auto

#[sweep.train.total_timesteps]
#distribution = log_normal
#min = 2e8
#max = 4e8
#mean = 2e8
#scale = time
