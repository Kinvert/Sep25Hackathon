[base]
package = ocean
env_name = puffer_drone_pp
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[vec]
num_envs = 8

[env]
num_envs = 16 # 16
num_drones = 64 # 64
max_rings = 10

penalty_damping = 0.065 #0.21 #0.1
reward_xy_dist = 0.09 #0.12 #0.05
reward_hover_dist = 0.88 #0.067 #0.15
reward_hover_alt = 0.031 #0.11 #0.03
reward_hover = 0.22 #0.2
reward_maint_hover = 0.12 #0.1
reward_descent = 0.75 #0.75
penalty_lost_hover = 0.075 #0.1
alignment = 0.001
min_alignment = 0.2
max_alignment = 0.001

reward_min_dist = 5.5
reward_max_dist = 65.0
dist_decay = 0.08

omega_prox = 0.04
stability_weight = 0.07

[train]
adam_beta1 = 0.9610890980775877
adam_beta2 = 0.9999260775286266
adam_eps = 7.782906079040132e-10
anneal_lr = true
batch_size = auto
bptt_horizon = 64
checkpoint_interval = 200
clip_coef = 0.05982655642208556
ent_coef = 0.002465076521024325
gae_lambda = 0.9641173414828333
gamma = 0.997472126425902
learning_rate = 0.010933756713881205
#learning_rate = 0.005
max_grad_norm = 1.6317688647793107
max_minibatch_size = 32768
minibatch_size = 32768
prio_alpha = 0.8968873016577552
prio_beta0 = 0.8672928227817938
total_timesteps = 200_000_000 # =====================================
update_epochs = 1
#use_rnn = false
vf_clip_coef = 0.5869845581530236
vf_coef = 2.1319065538539963
vtrace_c_clip = 2.714930379733876
vtrace_rho_clip = 3.8183814893708057

[sweep]
method = Protein
metric = dist100
goal = maximize
downsample = 0

#[sweep.env.omega_prox]
#distribution = uniform
#min = 0.001
#max = 0.2
#mean = 0.04
#scale = auto

#[sweep.env.stability_weight]
#distribution = uniform
#min = 0.001
#max = 0.2
#mean = 0.07
#scale = auto

#[sweep.env.reward_min_dist]
#distribution = uniform
#min = 0.1
#max = 10.0
#mean = 5.0
#scale = auto

#[sweep.env.reward_max_dist]
#distribution = uniform
#min = 10.0
#max = 100.0
#mean = 75.0
#scale = auto

#[sweep.env.dist_decay]
#distribution = uniform
#min = 0.01
#max = 0.5
#mean = 0.08
#scale = auto

#[sweep.env.alignment]
#distribution = uniform
#min = 0.0001
#max = 0.1
#mean = 0.001
#scale = auto

#[sweep.env.min_alignment]
#distribution = uniform
#min = 0.01
#max = 0.25
#mean = 0.2
#scale = auto

#[sweep.env.max_alignment]
#distribution = uniform
#min = 0.001
#max = 0.1
#mean = 0.01
#scale = auto

#[sweep.env.penalty_damping]
#distribution = uniform
#min = 0.001
#max = 0.5
#mean = 0.21
#scale = auto

#[sweep.env.reward_xy_dist]
#distribution = uniform
#min = 0.001
#max = 0.2
#mean = 0.12
#scale = auto

#[sweep.env.reward_hover_dist]
#distribution = uniform
#min = 0.001
#max = 0.2
#mean = 0.067
#scale = auto

#[sweep.env.reward_hover_alt]
#distribution = uniform
#min = 0.001
#max = 0.2
#mean = 0.11
#scale = auto

#[sweep.env.reward_hover]
#distribution = uniform
#min = 0.05
#max = 0.5
#mean = 0.2
#scale = auto

#[sweep.env.reward_maint_hover]
#distribution = uniform
#min = 0.05
#max = 0.2
#mean = 0.1
#scale = auto

#[sweep.env.reward_descent]
#distribution = uniform
#min = 0.2
#max = 1.0
#mean = 0.75
#scale = auto

#[sweep.env.penalty_lost_hover]
#distribution = uniform
#min = 0.01
#max = 0.2
#mean = 0.1
#scale = auto

#[sweep.train.total_timesteps]
#distribution = log_normal
#min = 2e8
#max = 4e8
#mean = 2e8
#scale = time
