[base]
package = ocean
env_name = puffer_drone_pp
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[vec]
num_envs = 24

[env]
num_envs = 24 # 16
num_drones = 64 # 64
max_rings = 10

box_base_density = 50.0
box_k_growth = 0.005

dist_decay = 0.45131343902919707

grip_k_decay = 0.05479766369222138
grip_k_max = 17.105300970916993
grip_k_min = 1.0

pos_const = 0.7108647198043252
pos_penalty = 0.0009629600280207475

reward_grip = 1.0
reward_ho_drop = 0.20890470430909128
reward_hover = 0.1603521816569735

reward_max_dist = 65.0
reward_min_dist = 0.9330297552248776

w_approach = 2.2462377881752698
w_position = 0.7312628607193232
w_stability = 1.6094417286807845
w_velocity = 0.0

[train]
adam_beta1 = 0.9739575783018397
adam_beta2 = 0.9983789918385146
adam_eps = 2.112852785895044e-07
anneal_lr = true
batch_size = auto
bptt_horizon = 64
checkpoint_interval = 200
clip_coef = 0.24983563859666713
ent_coef = 0.013899446116244659
gae_lambda = 0.995
gamma = 0.9650881439471051
learning_rate = 0.0033634189684819326
#learning_rate = 0.005
max_grad_norm = 2.0654635730488353
max_minibatch_size = 32768
minibatch_size = 8192
prio_alpha = 0.09999999999999998
prio_beta0 = 0.99
total_timesteps = 200_000_000
update_epochs = 1
#use_rnn = false
vf_clip_coef = 0.2650094882660451
vf_coef = 3.8658881803296214
vtrace_c_clip = 1.7651032878291006
vtrace_rho_clip = 2.0258848422642433

[sweep]
method = Protein
metric = perfect_deliv
goal = maximize
downsample = 0

[sweep.env.box_k_growth]
distribution = uniform
min = 0.005
max = 0.5
mean = 0.0051
scale = auto

[sweep.env.dist_decay]
distribution = uniform
min = 0.1
max = 1.0
mean = 0.45131343902919707
scale = auto

[sweep.env.grip_k_decay]
distribution = uniform
min = 0.01
max = 0.15
mean = 0.05479766369222138
scale = auto

[sweep.env.grip_k_max]
distribution = uniform
min = 10.0
max = 20.0
mean = 17.105300970916993
scale = auto

[sweep.env.pos_const]
distribution = uniform
min = 0.001
max = 1.0
mean = 0.7108647198043252
scale = auto

[sweep.env.pos_penalty]
distribution = uniform
min = 0.0001
max = 0.25
mean = 0.0009629600280207475
scale = auto

[sweep.env.reward_grip]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.9999
scale = auto

[sweep.env.reward_ho_drop]
distribution = uniform
min = 0.1
max = 0.5
mean = 0.20890470430909128
scale = auto

[sweep.env.reward_hover]
distribution = uniform
min = 0.0
max = 0.5
mean = 0.1603521816569735
scale = auto

[sweep.env.reward_min_dist]
distribution = uniform
min = 0.1
max = 5.0
mean = 0.9330297552248776
scale = auto

[sweep.env.w_approach]
distribution = uniform
min = 0.0
max = 2.5
mean = 2.2462377881752698
scale = auto

[sweep.env.w_position]
distribution = uniform
min = 0.0
max = 1.5
mean = 0.7312628607193232
scale = auto

[sweep.env.w_stability]
distribution = uniform
min = 0.0
max = 2.5
mean = 1.6094417286807845
scale = auto

[sweep.env.w_velocity]
distribution = uniform
min = 0.0
max = 1.5
mean = 0.001
scale = auto
