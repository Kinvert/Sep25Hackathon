[base]
package = ocean
env_name = puffer_drone_pp
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[vec]
num_envs = 24

[env]
num_envs = 24 # 16
num_drones = 64 # 64
max_rings = 10

reward_min_dist = 1.1105802559489502
reward_max_dist = 84.87905776253974
dist_decay = 0.4403467880504442

w_position = 0.4668049255345429
w_velocity = 0.12632002850721588
w_stability = 0.9204946924096309
w_approach = 2.2379216408855633

pos_const = 0.8016030890716737
pos_penalty = 0.001

grip_k_min = 1.0
grip_k_max = 13.333914053530759
grip_k_decay = 0.0834636071115364

box_base_density = 25.0
box_k_growth = 0.13614786264512518

reward_hover = 0.28340468806130464
reward_grip = 0.8346780437800967
reward_deliv = 0.8541015275237493

[train]
adam_beta1 = 0.7893498892909195
adam_beta2 = 0.9623029644746665
adam_eps = 7.638211589398773e-05
anneal_lr = true
batch_size = auto
bptt_horizon = 64
checkpoint_interval = 200
clip_coef = 0.42894856312066026
ent_coef = 0.008248551481341328
gae_lambda = 0.9898242318711233
gamma = 0.9790480028125498
learning_rate = 0.017051072679960887
#learning_rate = 0.005
max_grad_norm = 2.02126322788644
max_minibatch_size = 32768
minibatch_size = 8192
prio_alpha = 0.7353881668875533
prio_beta0 = 0.9564865329493006
total_timesteps = 200_000_000
update_epochs = 1
#use_rnn = false
vf_clip_coef = 0.1
vf_coef = 4.337952897580921
vtrace_c_clip = 4.337952897580921
vtrace_rho_clip = 2.7439830343099834

[sweep]
method = Protein
metric = perfect_deliv
goal = maximize
downsample = 0

[sweep.env.w_position]
distribution = uniform
min = 0.0
max = 1.5
mean = 1.23
scale = auto

[sweep.env.w_velocity]
distribution = uniform
min = 0.0
max = 1.5
mean = 0.13
scale = auto

[sweep.env.w_stability]
distribution = uniform
min = 0.0
max = 2.5
mean = 1.83
scale = auto

[sweep.env.w_approach]
distribution = uniform
min = 0.0
max = 2.5
mean = 2.4
scale = auto

[sweep.env.reward_min_dist]
distribution = uniform
min = 0.1
max = 5.0
mean = 1.6
scale = auto

[sweep.env.reward_max_dist]
distribution = uniform
min = 60.0
max = 100.0
mean = 77.0
scale = auto

[sweep.env.dist_decay]
distribution = uniform
min = 0.01
max = 0.5
mean = 0.5
scale = auto

[sweep.env.pos_const]
distribution = uniform
min = 0.001
max = 1.0
mean = 0.63
scale = auto

[sweep.env.pos_penalty]
distribution = uniform
min = 0.001
max = 0.25
mean = 0.04
scale = auto

[sweep.env.grip_k_max]
distribution = uniform
min = 1.0
max = 20.0
mean = 15.0
scale = auto

[sweep.env.grip_k_decay]
distribution = uniform
min = 0.01
max = 0.15
mean = 0.095
scale = auto

[sweep.env.box_base_density]
distribution = uniform
min = 25.0
max = 100.0
mean = 50.0
scale = auto

[sweep.env.box_k_growth]
distribution = uniform
min = 0.005
max = 0.5
mean = 0.02
scale = auto

[sweep.env.reward_hover]
distribution = uniform
min = 0.0
max = 0.5
mean = 0.25
scale = auto

[sweep.env.reward_grip]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = auto

[sweep.env.reward_deliv]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.75
scale = auto
